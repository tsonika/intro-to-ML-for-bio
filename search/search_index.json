{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Introduction to Machine Learning (ML) for Biologists Workshop The Introduction to ML for Biologists is short interactive workshop to introduce basic concepts of ML predictive modeling for genomics data. When and Where: Virtual, on Friday 14 th October at 9:00 to 11:30 (Istanbul standard time, GMT+3) | 11:30 -14:00 (Indian standard time) | 17:00 - 19:30 (Australian daylight saving time, GMT+11). Prerequisites: Familiarity with python (an intermediate level), specifically: Understand variables, data types, functions and classes Able to search for and install external libraries for own use cases Lead Instructor: Dr Sonika Tyagi, Monash University, AUSTRALIA Co-instructors/Helpers: Dr Harpreet Singh, Hansraj Mahila Maha Vidyalaya Punjab INDIA Tyrone Chen, and Naima Vahab, Tyagi Lab, Monash University Duration: 2.5 h Subject Time notes Date: 14 October 2022 Introductory lecture 30 min Slides talk Tokenization interactive demo 10 min Interactive demo Break 5 min ML models 1.5 h Hands-on Break 5 min Recap + QA 10 min Group discussion Workshop Content Topics covered by this workshop include: An introduction to ML concepts Transformation of genomics data for ML application Hands-on activities This workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions. Acknowledgements This workshop was developed by the memebers of the Tyagi lab at Monash University Australia. ALl authors have contributed equally","title":"Home"},{"location":"#welcome-to-the-introduction-to-machine-learning-ml-for-biologists-workshop","text":"The Introduction to ML for Biologists is short interactive workshop to introduce basic concepts of ML predictive modeling for genomics data.","title":"Welcome to the Introduction to Machine Learning (ML) for Biologists Workshop"},{"location":"#when-and-where","text":"Virtual, on Friday 14 th October at 9:00 to 11:30 (Istanbul standard time, GMT+3) | 11:30 -14:00 (Indian standard time) | 17:00 - 19:30 (Australian daylight saving time, GMT+11).","title":"When and Where:"},{"location":"#prerequisites","text":"Familiarity with python (an intermediate level), specifically: Understand variables, data types, functions and classes Able to search for and install external libraries for own use cases","title":"Prerequisites:"},{"location":"#lead-instructor","text":"Dr Sonika Tyagi, Monash University, AUSTRALIA","title":"Lead Instructor:"},{"location":"#co-instructorshelpers","text":"Dr Harpreet Singh, Hansraj Mahila Maha Vidyalaya Punjab INDIA Tyrone Chen, and Naima Vahab, Tyagi Lab, Monash University","title":"Co-instructors/Helpers:"},{"location":"#duration-25-h","text":"Subject Time notes Date: 14 October 2022 Introductory lecture 30 min Slides talk Tokenization interactive demo 10 min Interactive demo Break 5 min ML models 1.5 h Hands-on Break 5 min Recap + QA 10 min Group discussion","title":"Duration: 2.5 h"},{"location":"#workshop-content","text":"Topics covered by this workshop include: An introduction to ML concepts Transformation of genomics data for ML application Hands-on activities This workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions.","title":"Workshop Content"},{"location":"#acknowledgements","text":"This workshop was developed by the memebers of the Tyagi lab at Monash University Australia. ALl authors have contributed equally","title":"Acknowledgements"},{"location":"00_data_intro/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Recognize various genomics formats to represent DNA/RNA sequence data Read in sequence data in FASTA format using Python or BioPython modules decide on filters and cutoffs for cleaning up data ready for downstream analysis Resources You\u2019ll be Using Tools Used Python Package: http:// BioPython SeqIO package: https://biopython.org/wiki/SeqIO Google Colaboratory: http:// Useful Links FASTA format: https://www.ncbi.nlm.nih.gov/genbank/fastaformat FASTQ format: https://www.ncbi.nlm.nih.gov/sra/docs/submitformats/#fastq-files FASTQ Encoding: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding ) Author Information Primary Author(s): Sonika Tyagi: sonika.tyagi@monash.edu Contributor(s): Sarthak Chauhan, Navya Tyagi, Tyrone Chen FASTA format FASTQ format Quality Score Bins Mapped quality scores N (no call) N (no call) 2-9 6 10-19 15 20-24 22 25-29 27 30-34 33 35-39 37 >=40 40 Table 1: Novaseq Q-score bins mapping Reading in the sequence file in FASTA format A file containing one or more DNA or RNA squences in FASTA format can be read using one of the following two methods. Using a custom python script Open your Python notebook and type the following code in teh code cell and run. def parse_fasta ( fh ) : fs = [] fss = [] seq='' for ln in fh : if ln [ 0 ] =='A' or ln [ 0 ] =='T' or ln [ 0 ] =='G' or ln [ 0 ] =='C' : seq = seq + ln [ :- 2 ] else : fs . append ( seq ) seq='' for element in fs : if element ! ='' : fss . append ( element ) else : fss = fss return fss fh = open ( '/Sequence.fasta' ) input = parse_fasta ( fh ) print ( input ) Using Biopython utilities from Bio import SeqIO for record in SeqIO.parse(\"example.fasta\", \"fasta\"): print(record.id)","title":"Data Introduction"},{"location":"00_data_intro/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Recognize various genomics formats to represent DNA/RNA sequence data Read in sequence data in FASTA format using Python or BioPython modules decide on filters and cutoffs for cleaning up data ready for downstream analysis","title":"Key Learning Outcomes"},{"location":"00_data_intro/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"00_data_intro/handout/handout/#tools-used","text":"Python Package: http:// BioPython SeqIO package: https://biopython.org/wiki/SeqIO Google Colaboratory: http://","title":"Tools Used"},{"location":"00_data_intro/handout/handout/#useful-links","text":"FASTA format: https://www.ncbi.nlm.nih.gov/genbank/fastaformat FASTQ format: https://www.ncbi.nlm.nih.gov/sra/docs/submitformats/#fastq-files FASTQ Encoding: ( http://en.wikipedia.org/wiki/FASTQ_format#Encoding )","title":"Useful Links"},{"location":"00_data_intro/handout/handout/#author-information","text":"Primary Author(s): Sonika Tyagi: sonika.tyagi@monash.edu Contributor(s): Sarthak Chauhan, Navya Tyagi, Tyrone Chen","title":"Author Information"},{"location":"00_data_intro/handout/handout/#fasta-format","text":"","title":"FASTA format"},{"location":"00_data_intro/handout/handout/#fastq-format","text":"Quality Score Bins Mapped quality scores N (no call) N (no call) 2-9 6 10-19 15 20-24 22 25-29 27 30-34 33 35-39 37 >=40 40 Table 1: Novaseq Q-score bins mapping","title":"FASTQ format"},{"location":"00_data_intro/handout/handout/#reading-in-the-sequence-file-in-fasta-format","text":"A file containing one or more DNA or RNA squences in FASTA format can be read using one of the following two methods. Using a custom python script Open your Python notebook and type the following code in teh code cell and run. def parse_fasta ( fh ) : fs = [] fss = [] seq='' for ln in fh : if ln [ 0 ] =='A' or ln [ 0 ] =='T' or ln [ 0 ] =='G' or ln [ 0 ] =='C' : seq = seq + ln [ :- 2 ] else : fs . append ( seq ) seq='' for element in fs : if element ! ='' : fss . append ( element ) else : fss = fss return fss fh = open ( '/Sequence.fasta' ) input = parse_fasta ( fh ) print ( input ) Using Biopython utilities from Bio import SeqIO for record in SeqIO.parse(\"example.fasta\", \"fasta\"): print(record.id)","title":"Reading in the sequence file in FASTA format"},{"location":"01_tokenization/handout/handout/","text":"Preprocessing Biological Sequence Data for Natural Language Processing Learning objectives: The overall objective is to understand how biological sequence data is transformed into a machine-readable format for input into a machine learning pipeline. View the flow of data through a conventional preproceessing pipeline Learn common data transformations Familiarity with common representations Explore how large sequences are separated into smaller components Awareness of the ecosystem of available libraries Resources You\u2019ll be Using Tools Used Python Package: http:// BioPython SeqIO package: https://biopython.org/wiki/SeqIO Google Colaboratory: http:// Useful Links Author Information Primary Author(s): Tyrone Chen, Navya Tyagi, Naima Vahab Contributor(s): Sonika Tyagi, Sarthak Chauhan Workflow Assuming you have obtained well-defined sequence data and their corresponding metadata: 1. Split each input sequence into \u201ck-mers\u201d or \u201ctokens\u201d. 2. Denoise the data if needed. 3. Convert these tokens into a numeric representation. 4. Transform the numeric representation as needed, depending on your use case. NOTE : in the context of this tutorial, we will be using the term token and k-mer interchangeably. Let us take a single biological sequence as an example. Assume that this corresponds to a gene of interest: input_sequence = \"TAATGGCTCT\" 1) Split each input sequence into \u201ctokens\u201d. It is impractical in most cases for the machine to obtain a representation of a whole sequence, for example a whole document or chromosome. On the other hand, the same is true for obtaining individual letters of text or genomic DNA. While in the case of some languages such as English, it is possible to exploit the property of space-separation, this is not applicable to genomic DNA. Therefore, in most cases an arbitrary-length split is performed. def get_kmers(sequence: str, length: int): \"\"\" Take a dna sequence as input and split the sequence into k-mers / tokens \"\"\" return [sequence[x:x+length].upper() for x in range(len(sequence) - length + 1)] # in this example, we will split on length 3 k-mers length = 3 kmers = get_kmers(input_sequence, length) print(kmers) 2) Denoise the data if needed In some cases, we want to remove certain tokens, commonly tokens with low information content. In English, this is straightforward and involves filtering out \u201cstopwords\u201d, for example the , and or is . In Biology, this is not always done, and will require review on a case-by-case basis. def filter_kmers(tokens: str, stopwords: list): \"\"\" Take an input dna sequence and list of stopwords to remove from the data. \"\"\" return [x for x in tokens if x not in stopwords] # in this example, let us pretend this list of k-mers have low information content stopwords = [\"TAA\", \"AAT\"] filtered_kmers = filter_kmers(kmers, stopwords) print(filtered_kmers) Step 1: Numeric encoding Ordinal encoding One-hot encoding The first type of text encodings are ordinal encodings. With this, individual tokens are assigned an integer value as shown below: from sklearn.preprocessing import OrdinalEncoder def encode_ordinal(input_sequence: str, length: int): \"\"\" Take a list of k-mers and perform ordinal encoding \"\"\" tokens = [[x] for x in get_kmers(input_sequence, length)] encoder = OrdinalEncoder() return encoder.fit_transform(tokens) ordinal = encode_ordinal(input_sequence, length) print(ordinal) NOTE : As an alternative to ordinal encoding, it is also possible to encode tokens as one-hot encodings. This method is not practical to use in most cases due to its sparsity, but we demonstrate an example for completeness. Compare this to the above: [[ 0 0 0 0 0 1 0 0] [ 1 0 0 0 0 0 0 0] [ 0 1 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 1] [ 0 0 0 0 1 0 0 0] [ 0 0 0 1 0 0 0 0] [ 0 0 1 0 0 0 0 0] [ 0 0 0 0 0 0 1 0]] While these can be used on their own, note that this is often just the first step, and that ordinal encodings can be transformed further. While these can be used on their own, note that this is often just the first step, and that ordinal encodings can be transformed further. Step 2: Transformation Frequency tables Embedding vector Positional encoding Frequency tables Count vectorisation First, each token is counted and these values are assigned to a dictionary. On its own, these counts can be vectorised directly. However, it is often practical to weight tokens by their frequency as shown in the next step: TF-IDF (Term Frequency-Inverse Document Frequency) TF-IDF is analogous to a library-size correction in conventional short-read RNA-Seq, allowing different size sequence collections to be compared on similar scales. It is computed on the previous counts prior to vectorisation. TF-IDF can be calculated as follows: tf-idf(t, d) = tf(t, d) * log(N/(df + 1)) Where: - t = token - d = sequence - N = total number of sequences in the library Within scikit-learn , the counting process is abstracted and not shown directly. We demonstrate this intermediate step for reference: # input 'TAA AAT ATG TGG GGC GCT CTC TCT' # intermediate (unweighted matrix) {'TAA': 1, 'AAT': 1, 'ATG': 1, 'TGG': 1, 'GGC': 1, 'GCT': 1, 'CTC': 1, 'TCT': 1} # resulting matrix from sklearn.feature_extraction.text import TfidfVectorizer def tfidf(input_sequence: str, length: int): \"\"\" Take a dna sequence and k-mer size as input, output a matrix of TF-IDF features. \"\"\" data = [\" \".join(get_kmers(input_sequence, length))] tfidf = TfidfVectorizer() tfidf = tfidf.fit_transform(data) return tfidf.toarray() tfidf_matrix = tfidf(input_sequence,length) print(tfidf_matrix) Although the frequency-based approaches above are often effective on their own, one key piece of information is lost. Individual tokens are treated as independent occurrences, which is not realistic in both natural language and biology. To account for the semantic relationship between tokens, a different approach is required. Embedding vector A latent space is generated from all tokens in the corpus. This n-dimensional embedding accounts for every combination of tokens in the data. Taking a single sequence (i.e. group of tokens) and projecting this back onto the embedding returns a unique vector for each sequence. Examining these further shows that tokens which are closely related to each other are naturally recapitulated by the model. For example, in a real life English dataset, you may expect the following word pairs to be closely related to each other: IMAGE HERE import gensim from gensim.models import Word2Vec def create_word2vec(input_sequence: str, length: int): \"\"\" Input a list of tokens to generate an embedding of word vectors \"\"\" data = get_kmers(input_sequence, length) return Word2Vec([data], min_count=1) def map_token(input_sequence: str, model: gensim.models.base_any2vec): \"\"\" Input a token and embedding and map the token onto the embedding \"\"\" return model[input_sequence] embedding = create_word2vec(input_sequence, length) vector = map_token('TAA', embedding) print(vector) While we demonstrate gensim as one example, many embedding algorithms exist. A comprehensive overview of each method is out of the scope of this tutorial, but we list a few popular methods for your reference: Word2Vec dna2vec (dna equivalent of the above) GloVe END","title":"Tokenization"},{"location":"01_tokenization/handout/handout/#preprocessing-biological-sequence-data-for-natural-language-processing","text":"","title":"Preprocessing Biological Sequence Data for Natural Language Processing"},{"location":"01_tokenization/handout/handout/#learning-objectives","text":"The overall objective is to understand how biological sequence data is transformed into a machine-readable format for input into a machine learning pipeline. View the flow of data through a conventional preproceessing pipeline Learn common data transformations Familiarity with common representations Explore how large sequences are separated into smaller components Awareness of the ecosystem of available libraries","title":"Learning objectives:"},{"location":"01_tokenization/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"01_tokenization/handout/handout/#tools-used","text":"Python Package: http:// BioPython SeqIO package: https://biopython.org/wiki/SeqIO Google Colaboratory: http://","title":"Tools Used"},{"location":"01_tokenization/handout/handout/#useful-links","text":"","title":"Useful Links"},{"location":"01_tokenization/handout/handout/#author-information","text":"Primary Author(s): Tyrone Chen, Navya Tyagi, Naima Vahab Contributor(s): Sonika Tyagi, Sarthak Chauhan","title":"Author Information"},{"location":"01_tokenization/handout/handout/#workflow","text":"Assuming you have obtained well-defined sequence data and their corresponding metadata: 1. Split each input sequence into \u201ck-mers\u201d or \u201ctokens\u201d. 2. Denoise the data if needed. 3. Convert these tokens into a numeric representation. 4. Transform the numeric representation as needed, depending on your use case. NOTE : in the context of this tutorial, we will be using the term token and k-mer interchangeably. Let us take a single biological sequence as an example. Assume that this corresponds to a gene of interest: input_sequence = \"TAATGGCTCT\"","title":"Workflow"},{"location":"01_tokenization/handout/handout/#1-split-each-input-sequence-into-tokens","text":"It is impractical in most cases for the machine to obtain a representation of a whole sequence, for example a whole document or chromosome. On the other hand, the same is true for obtaining individual letters of text or genomic DNA. While in the case of some languages such as English, it is possible to exploit the property of space-separation, this is not applicable to genomic DNA. Therefore, in most cases an arbitrary-length split is performed. def get_kmers(sequence: str, length: int): \"\"\" Take a dna sequence as input and split the sequence into k-mers / tokens \"\"\" return [sequence[x:x+length].upper() for x in range(len(sequence) - length + 1)] # in this example, we will split on length 3 k-mers length = 3 kmers = get_kmers(input_sequence, length) print(kmers)","title":"1) Split each input sequence into \"tokens\"."},{"location":"01_tokenization/handout/handout/#2-denoise-the-data-if-needed","text":"In some cases, we want to remove certain tokens, commonly tokens with low information content. In English, this is straightforward and involves filtering out \u201cstopwords\u201d, for example the , and or is . In Biology, this is not always done, and will require review on a case-by-case basis. def filter_kmers(tokens: str, stopwords: list): \"\"\" Take an input dna sequence and list of stopwords to remove from the data. \"\"\" return [x for x in tokens if x not in stopwords] # in this example, let us pretend this list of k-mers have low information content stopwords = [\"TAA\", \"AAT\"] filtered_kmers = filter_kmers(kmers, stopwords) print(filtered_kmers)","title":"2) Denoise the data if needed"},{"location":"01_tokenization/handout/handout/#step-1-numeric-encoding","text":"Ordinal encoding One-hot encoding The first type of text encodings are ordinal encodings. With this, individual tokens are assigned an integer value as shown below: from sklearn.preprocessing import OrdinalEncoder def encode_ordinal(input_sequence: str, length: int): \"\"\" Take a list of k-mers and perform ordinal encoding \"\"\" tokens = [[x] for x in get_kmers(input_sequence, length)] encoder = OrdinalEncoder() return encoder.fit_transform(tokens) ordinal = encode_ordinal(input_sequence, length) print(ordinal) NOTE : As an alternative to ordinal encoding, it is also possible to encode tokens as one-hot encodings. This method is not practical to use in most cases due to its sparsity, but we demonstrate an example for completeness. Compare this to the above: [[ 0 0 0 0 0 1 0 0] [ 1 0 0 0 0 0 0 0] [ 0 1 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 1] [ 0 0 0 0 1 0 0 0] [ 0 0 0 1 0 0 0 0] [ 0 0 1 0 0 0 0 0] [ 0 0 0 0 0 0 1 0]] While these can be used on their own, note that this is often just the first step, and that ordinal encodings can be transformed further. While these can be used on their own, note that this is often just the first step, and that ordinal encodings can be transformed further.","title":"Step 1: Numeric encoding"},{"location":"01_tokenization/handout/handout/#step-2-transformation","text":"Frequency tables Embedding vector Positional encoding","title":"Step 2: Transformation"},{"location":"01_tokenization/handout/handout/#frequency-tables","text":"Count vectorisation First, each token is counted and these values are assigned to a dictionary. On its own, these counts can be vectorised directly. However, it is often practical to weight tokens by their frequency as shown in the next step: TF-IDF (Term Frequency-Inverse Document Frequency) TF-IDF is analogous to a library-size correction in conventional short-read RNA-Seq, allowing different size sequence collections to be compared on similar scales. It is computed on the previous counts prior to vectorisation. TF-IDF can be calculated as follows: tf-idf(t, d) = tf(t, d) * log(N/(df + 1)) Where: - t = token - d = sequence - N = total number of sequences in the library Within scikit-learn , the counting process is abstracted and not shown directly. We demonstrate this intermediate step for reference: # input 'TAA AAT ATG TGG GGC GCT CTC TCT' # intermediate (unweighted matrix) {'TAA': 1, 'AAT': 1, 'ATG': 1, 'TGG': 1, 'GGC': 1, 'GCT': 1, 'CTC': 1, 'TCT': 1} # resulting matrix from sklearn.feature_extraction.text import TfidfVectorizer def tfidf(input_sequence: str, length: int): \"\"\" Take a dna sequence and k-mer size as input, output a matrix of TF-IDF features. \"\"\" data = [\" \".join(get_kmers(input_sequence, length))] tfidf = TfidfVectorizer() tfidf = tfidf.fit_transform(data) return tfidf.toarray() tfidf_matrix = tfidf(input_sequence,length) print(tfidf_matrix) Although the frequency-based approaches above are often effective on their own, one key piece of information is lost. Individual tokens are treated as independent occurrences, which is not realistic in both natural language and biology. To account for the semantic relationship between tokens, a different approach is required.","title":"Frequency tables"},{"location":"01_tokenization/handout/handout/#embedding-vector","text":"A latent space is generated from all tokens in the corpus. This n-dimensional embedding accounts for every combination of tokens in the data. Taking a single sequence (i.e. group of tokens) and projecting this back onto the embedding returns a unique vector for each sequence. Examining these further shows that tokens which are closely related to each other are naturally recapitulated by the model. For example, in a real life English dataset, you may expect the following word pairs to be closely related to each other:","title":"Embedding vector"},{"location":"01_tokenization/handout/handout/#image-here","text":"import gensim from gensim.models import Word2Vec def create_word2vec(input_sequence: str, length: int): \"\"\" Input a list of tokens to generate an embedding of word vectors \"\"\" data = get_kmers(input_sequence, length) return Word2Vec([data], min_count=1) def map_token(input_sequence: str, model: gensim.models.base_any2vec): \"\"\" Input a token and embedding and map the token onto the embedding \"\"\" return model[input_sequence] embedding = create_word2vec(input_sequence, length) vector = map_token('TAA', embedding) print(vector) While we demonstrate gensim as one example, many embedding algorithms exist. A comprehensive overview of each method is out of the scope of this tutorial, but we list a few popular methods for your reference: Word2Vec dna2vec (dna equivalent of the above) GloVe","title":"IMAGE HERE"},{"location":"01_tokenization/handout/handout/#end","text":"","title":"END"}]}