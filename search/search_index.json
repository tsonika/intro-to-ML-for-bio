{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Introduction to Machine Learning (ML) for Bioinformatics Workshop The Introduction to ML is short interactive workshop to introduce basic concepts of ML predictive modeling for genomics data. When and Where: Virtual, on Sunday 13 November 2022 Prerequisites: Familiarity with python (an intermediate level), specifically: Understand variables, data types, functions and classes Able to search for and install external libraries for own use cases Lead Instructor: Dr Sonika Tyagi, Monash University, AUSTRALIA Co-instructors/Helpers: Dr Harpreet Singh, Hansraj Mahila Maha Vidyalaya Punjab INDIA Tyrone Chen, and Naima Vahab, Tyagi Lab, Monash University Duration: 2.0 h Subject Time notes Date: 13 November 2022 Introductory lecture 40 min Slides talk Break 5 min ML models 1 h Hands-on Break 5 min Recap + QA 10 min Group discussion Workshop Content Topics covered by this workshop include: An introduction to ML concepts Transformation of genomics data for ML application Hands-on activities This workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions. Acknowledgements This workshop was developed by the memebers of the Tyagi lab at Monash University Australia. All authors have contributed equally The templates used for the ghpages were developed as part of the BPA Australia Ltd training activities.","title":"Home"},{"location":"#welcome-to-the-introduction-to-machine-learning-ml-for-bioinformatics-workshop","text":"The Introduction to ML is short interactive workshop to introduce basic concepts of ML predictive modeling for genomics data.","title":"Welcome to the Introduction to Machine Learning (ML) for Bioinformatics Workshop"},{"location":"#when-and-where","text":"Virtual, on Sunday 13 November 2022","title":"When and Where:"},{"location":"#prerequisites","text":"Familiarity with python (an intermediate level), specifically: Understand variables, data types, functions and classes Able to search for and install external libraries for own use cases","title":"Prerequisites:"},{"location":"#lead-instructor","text":"Dr Sonika Tyagi, Monash University, AUSTRALIA","title":"Lead Instructor:"},{"location":"#co-instructorshelpers","text":"Dr Harpreet Singh, Hansraj Mahila Maha Vidyalaya Punjab INDIA Tyrone Chen, and Naima Vahab, Tyagi Lab, Monash University","title":"Co-instructors/Helpers:"},{"location":"#duration-20-h","text":"Subject Time notes Date: 13 November 2022 Introductory lecture 40 min Slides talk Break 5 min ML models 1 h Hands-on Break 5 min Recap + QA 10 min Group discussion","title":"Duration: 2.0 h"},{"location":"#workshop-content","text":"Topics covered by this workshop include: An introduction to ML concepts Transformation of genomics data for ML application Hands-on activities This workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions.","title":"Workshop Content"},{"location":"#acknowledgements","text":"This workshop was developed by the memebers of the Tyagi lab at Monash University Australia. All authors have contributed equally The templates used for the ghpages were developed as part of the BPA Australia Ltd training activities.","title":"Acknowledgements"},{"location":"license/","text":"This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full license) available at http://creativecommons.org/licenses/by/3.0/legalcode . You are free: to copy, distribute, display, and perform the work to make derivative works to make commercial use of the work Under the following conditions: Attribution - You must give the original author credit. With the understanding that: Waiver - Any of the above conditions can be waived if you get permission from the copyright holder. Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license. Other Rights - In no way are any of the following rights affected by the license: Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations; The author\u2019s moral rights; Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights. Notice - For any reuse or distribution, you must make clear to others the license terms of this work.","title":"License"},{"location":"00_data_intro/handout/handout/","text":"Key Learning Outcomes After completing this practical the trainee should be able to: Recognize various genomics formats to represent DNA/RNA sequence data Read in sequence data in FASTA format using Python or BioPython modules Resources You\u2019ll be Using Tools Used BioPython SeqIO package: https://biopython.org/wiki/SeqIO https://biopython.org/docs/1.75/api/Bio.SeqIO.QualityIO.html Google Colaboratory: https://colab.research.google.com/ Useful Links FASTA format: https://www.ncbi.nlm.nih.gov/genbank/fastaformat FASTQ format: https://www.ncbi.nlm.nih.gov/sra/docs/submitformats/#fastq-files Author Information Author(s): Sonika Tyagi, Sarthak Chauhan, Navya Tyagi, Tyrone Chen DNA sequencing of a biological specimen is performed to find the order of letters (or bases) A, C, G, and T in it to determine the code of DNA. Different organisms have DNA of different length, for example, human DNA is made of 3 billion pairs of individual bases. A highthroughput DNA sequencing experiment can generate miliions of copies of DNA sequences and these are provided as long text files. There are standard formats for presenting DNA sequence data. Two of the most common primary DNA data format are the FASTA (stands for FAST alignment) and FASTQ format. FASTA format A FASTA format of biological sequence has mainly two fields: a unique sequence identifier line that starts with a > sign and the actuall sequence begins from the seconf line. >My_seq_ID TAATGGCTCT GGAAGCTCT TGGCTCTAGA FASTQ format Raw data from a highthroughput DNA sequencer is commonly stored in a FASTQ format. This format is similar to FASTA except it contains 4 different fiels and additionally have DNA quality information in it. Here, the sequence identifier line starts with an @ sign, followed by the actual sequence, a line starting + can optionally ahve metadata informaiton, and finally a quality string of the ssame length as the DNA sequence. @SRR001666.1 071112_SLXA-EAS1_s_7:5:1:817:345 length=36 GGGTGATGGCCGCTGCCGATGGCGTCAAATCCCACC +SRR001666.1 071112_SLXA-EAS1_s_7:5:1:817:345 length=36 IIIIIIIIIIIIIIIIIIIIIIIIIIIIII9IG9IC The fouurth line int eh above format is a quality string where each letter represent a PHRED scale DNA base quality score reperesented on a scale of 0-40+ . The table below provides an indication to how quality scores related to base call accuracies. Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999% Table 1: Phred quality scores are logarithmically linked to error probabilities Reading in the sequence file in FASTA format A file containing one or more DNA or RNA squences in FASTA format can be read using one of the following two methods. 1) Using a custom python script Open your Python notebook and type the following code in teh code cell and run. def parse_fasta ( fh ) : fs = [] fss = [] seq='' for ln in fh : if ln [ 0 ] =='A' or ln [ 0 ] =='T' or ln [ 0 ] =='G' or ln [ 0 ] =='C': seq = seq + ln [:- 2 ] else : fs . append ( seq ) seq='' for element in fs : if element ! ='': fss . append ( element ) else : fss = fss return fss fh = open ( '/Sequence.fasta' ) input = parse_fasta ( fh ) print ( input ) 2) Using Biopython utilities from Bio import SeqIO for sequence in SeqIO.parse(\"example.fasta\", \"fasta\"): print(sequence.id) print(sequence.seq) print(len(sequence) A similar function is available to read the FASTQ format as well: from Bio import SeqIO for record in SeqIO.parse(\"example.fastq\", \"fastq\"): print(\"%s %s\" % (record.id, record.seq)) Output will look like this: SeqID1 CCCTTCTTGTCTTCAGCGTTTCTCC SeqID2 TTGGCAGGCCAAGGCCGATGGATCA SeqID3 GTTGCTTCTGGCGTGGGTGGGGGGG","title":"Data Introduction"},{"location":"00_data_intro/handout/handout/#key-learning-outcomes","text":"After completing this practical the trainee should be able to: Recognize various genomics formats to represent DNA/RNA sequence data Read in sequence data in FASTA format using Python or BioPython modules","title":"Key Learning Outcomes"},{"location":"00_data_intro/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"00_data_intro/handout/handout/#tools-used","text":"BioPython SeqIO package: https://biopython.org/wiki/SeqIO https://biopython.org/docs/1.75/api/Bio.SeqIO.QualityIO.html Google Colaboratory: https://colab.research.google.com/","title":"Tools Used"},{"location":"00_data_intro/handout/handout/#useful-links","text":"FASTA format: https://www.ncbi.nlm.nih.gov/genbank/fastaformat FASTQ format: https://www.ncbi.nlm.nih.gov/sra/docs/submitformats/#fastq-files","title":"Useful Links"},{"location":"00_data_intro/handout/handout/#author-information","text":"Author(s): Sonika Tyagi, Sarthak Chauhan, Navya Tyagi, Tyrone Chen DNA sequencing of a biological specimen is performed to find the order of letters (or bases) A, C, G, and T in it to determine the code of DNA. Different organisms have DNA of different length, for example, human DNA is made of 3 billion pairs of individual bases. A highthroughput DNA sequencing experiment can generate miliions of copies of DNA sequences and these are provided as long text files. There are standard formats for presenting DNA sequence data. Two of the most common primary DNA data format are the FASTA (stands for FAST alignment) and FASTQ format.","title":"Author Information"},{"location":"00_data_intro/handout/handout/#fasta-format","text":"A FASTA format of biological sequence has mainly two fields: a unique sequence identifier line that starts with a > sign and the actuall sequence begins from the seconf line. >My_seq_ID TAATGGCTCT GGAAGCTCT TGGCTCTAGA","title":"FASTA format"},{"location":"00_data_intro/handout/handout/#fastq-format","text":"Raw data from a highthroughput DNA sequencer is commonly stored in a FASTQ format. This format is similar to FASTA except it contains 4 different fiels and additionally have DNA quality information in it. Here, the sequence identifier line starts with an @ sign, followed by the actual sequence, a line starting + can optionally ahve metadata informaiton, and finally a quality string of the ssame length as the DNA sequence. @SRR001666.1 071112_SLXA-EAS1_s_7:5:1:817:345 length=36 GGGTGATGGCCGCTGCCGATGGCGTCAAATCCCACC +SRR001666.1 071112_SLXA-EAS1_s_7:5:1:817:345 length=36 IIIIIIIIIIIIIIIIIIIIIIIIIIIIII9IG9IC The fouurth line int eh above format is a quality string where each letter represent a PHRED scale DNA base quality score reperesented on a scale of 0-40+ . The table below provides an indication to how quality scores related to base call accuracies. Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999% Table 1: Phred quality scores are logarithmically linked to error probabilities","title":"FASTQ format"},{"location":"00_data_intro/handout/handout/#reading-in-the-sequence-file-in-fasta-format","text":"A file containing one or more DNA or RNA squences in FASTA format can be read using one of the following two methods.","title":"Reading in the sequence file in FASTA format"},{"location":"00_data_intro/handout/handout/#1-using-a-custom-python-script","text":"Open your Python notebook and type the following code in teh code cell and run. def parse_fasta ( fh ) : fs = [] fss = [] seq='' for ln in fh : if ln [ 0 ] =='A' or ln [ 0 ] =='T' or ln [ 0 ] =='G' or ln [ 0 ] =='C': seq = seq + ln [:- 2 ] else : fs . append ( seq ) seq='' for element in fs : if element ! ='': fss . append ( element ) else : fss = fss return fss fh = open ( '/Sequence.fasta' ) input = parse_fasta ( fh ) print ( input )","title":"1) Using a custom python script"},{"location":"00_data_intro/handout/handout/#2-using-biopython-utilities","text":"from Bio import SeqIO for sequence in SeqIO.parse(\"example.fasta\", \"fasta\"): print(sequence.id) print(sequence.seq) print(len(sequence) A similar function is available to read the FASTQ format as well: from Bio import SeqIO for record in SeqIO.parse(\"example.fastq\", \"fastq\"): print(\"%s %s\" % (record.id, record.seq)) Output will look like this: SeqID1 CCCTTCTTGTCTTCAGCGTTTCTCC SeqID2 TTGGCAGGCCAAGGCCGATGGATCA SeqID3 GTTGCTTCTGGCGTGGGTGGGGGGG","title":"2) Using Biopython utilities"},{"location":"01_tokenization/handout/handout/","text":"Preprocessing Biological Sequence Data for Natural Language Processing Learning objectives: The overall objective is to understand how biological sequence data is transformed into a machine-readable format for input into a machine learning pipeline. View the flow of data through a conventional DNA data preproceessing pipeline Familiarity with common genomic data transformations and representations Explore how large sequences are separated into smaller components Awareness of the ecosystem of available libraries Resources You\u2019ll be Using Tools Used Python Package: Pandas, BioPython, sklearn, gensim BioPython SeqIO package: https://biopython.org/wiki/SeqIO Google Colaboratory: https://colab.research.google.com/ Useful Links Author Information Primary Author(s): Tyrone Chen, Navya Tyagi, Naima Vahab Contributor(s): Sonika Tyagi Workflow Assuming you have obtained well-defined sequence data and their corresponding metadata: 1. Split each input sequence into \u201ck-mers\u201d or \u201ctokens\u201d. 2. Denoise the data if needed. 3. Convert these tokens into a numeric representation. 4. Transform the numeric representation as needed, depending on your use case. NOTE : in the context of this tutorial, we will be using the term token and k-mer interchangeably. Let us take a single biological sequence as an example. Assume that this corresponds to a gene of interest: input_sequence = \"TAATGGCTCT\" 1) Split each input sequence into \u201ctokens\u201d. It is impractical in most cases for the machine to obtain a representation of a whole sequence, for example a whole document or chromosome. On the other hand, the same is true for obtaining individual letters of text or genomic DNA. While in the case of some languages such as English, it is possible to exploit the property of space-separation, this is not applicable to genomic DNA. Therefore, in most cases an arbitrary-length split is performed. def get_kmers(sequence: str, length: int): \"\"\" Take a dna sequence as input and split the sequence into k-mers / tokens \"\"\" return [sequence[x:x+length].upper() for x in range(len(sequence) - length + 1)] # in this example, we will split on length 3 k-mers length = 3 kmers = get_kmers(input_sequence, length) print(kmers) We can see that we get k-mers or tokens of length 3 in our example. It is important to note that we take a sliding window of k-mers: INPUT : TAATG... KMER_1: TAA-- KMER_2: -AAT- KMER_3: --ATG NOTE : in real applications, a range of longer k-mers are commonly used, and this is not the only way to split your data into chunks0 2) Denoise the data if needed In some cases, we want to remove certain tokens, commonly tokens with low information content. In English, this is straightforward and involves filtering out \u201cstopwords\u201d, for example the , and or is . In Biology, this is not always done, and will require review on a case-by-case basis. def filter_kmers(tokens: str, stopwords: list): \"\"\" Take an input dna sequence and list of stopwords to remove from the data. \"\"\" return [x for x in tokens if x not in stopwords] # in this example, let us pretend this list of k-mers have low information content stopwords = [\"TAA\", \"AAT\"] filtered_kmers = filter_kmers(kmers, stopwords) print(filtered_kmers) Step 1: Numeric encoding Ordinal encoding One-hot encoding The first type of text encodings are ordinal encodings. With this, individual tokens are assigned an integer value as shown below: from sklearn.preprocessing import OrdinalEncoder def encode_ordinal(input_sequence: str, length: int): \"\"\" Take a list of k-mers and perform ordinal encoding \"\"\" tokens = [[x] for x in get_kmers(input_sequence, length)] encoder = OrdinalEncoder() return encoder.fit_transform(tokens) ordinal = encode_ordinal(input_sequence, length) print(ordinal) [[5.] [0.] [1.] [7.] [4.] [3.] [2.] [6.]] NOTE : As an alternative to ordinal encoding, it is also possible to encode tokens as one-hot encodings. This method is not practical to use in most cases due to its sparsity, but we demonstrate an example for completeness. Compare this to the above: from numpy import asarray from sklearn.preprocessing import OneHotEncoder def onehoten(sequence,size): r\"\"\"Array of categories(here string) are sorted and returns binary variables for each category\"\"\" doc=sentence_in_list(sequence,size) data = asarray(doc) encoder = OneHotEncoder(sparse=False) onehot = encoder.fit_transform(data) return onehot onehoten(input_sequence,length) [[ 0 0 0 0 0 1 0 0] [ 1 0 0 0 0 0 0 0] [ 0 1 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 1] [ 0 0 0 0 1 0 0 0] [ 0 0 0 1 0 0 0 0] [ 0 0 1 0 0 0 0 0] [ 0 0 0 0 0 0 1 0]] While these can be used on their own, note that this is often just the first step, and that ordinal encodings can be transformed further. While these can be used on their own, note that this is often just the first step, and that ordinal encodings can be transformed further. Step 2: Transformation Frequency tables Embedding vector Positional encoding Frequency tables Count vectorisation First, each token is counted and these values are assigned to a dictionary. On its own, these counts can be vectorised directly. However, it is often practical to weight tokens by their frequency as shown in the next step: TF-IDF (Term Frequency-Inverse Document Frequency) TF-IDF is analogous to a library-size correction in conventional short-read RNA-Seq, allowing different size sequence collections to be compared on similar scales. It is computed on the previous counts prior to vectorisation. TF-IDF can be calculated as follows: tf-idf(t, d) = tf(t, d) * log(N/(df + 1)) Where: - t = token - d = sequence - N = total number of sequences in the library Within scikit-learn , the counting process is abstracted and not shown directly. We demonstrate this intermediate step for reference: # input 'TAA AAT ATG TGG GGC GCT CTC TCT' # intermediate (unweighted matrix) {'TAA': 1, 'AAT': 1, 'ATG': 1, 'TGG': 1, 'GGC': 1, 'GCT': 1, 'CTC': 1, 'TCT': 1} # resulting matrix from sklearn.feature_extraction.text import TfidfVectorizer def tfidf(input_sequence: str, length: int): \"\"\" Take a dna sequence and k-mer size as input, output a matrix of TF-IDF features. \"\"\" data = [\" \".join(get_kmers(input_sequence, length))] tfidf = TfidfVectorizer() tfidf = tfidf.fit_transform(data) return tfidf.toarray() tfidf_matrix = tfidf(input_sequence,length) print(tfidf_matrix) Although the frequency-based approaches above are often effective on their own, one key piece of information is lost. Individual tokens are treated as independent occurrences, which is not realistic in both natural language and biology. To account for the semantic relationship between tokens, a different approach is required. Embedding vector A latent space is generated from all tokens in the corpus. This n-dimensional embedding accounts for every combination of tokens in the data. Taking a single sequence (i.e. group of tokens) and projecting this back onto the embedding returns a unique vector for each sequence. Examining these further shows that tokens which are closely related to each other are naturally recapitulated by the model. For example, in a real life English dataset, you may expect the following word pairs to be closely related to each other: import gensim from gensim.models import Word2Vec def create_word2vec(input_sequence: str, length: int): \"\"\" Input a list of tokens to generate an embedding of word vectors \"\"\" data = get_kmers(input_sequence, length) return Word2Vec([data], min_count=1) def map_token(input_sequence: str, model: gensim.models.base_any2vec): \"\"\" Input a token and embedding and map the token onto the embedding \"\"\" return model[input_sequence] embedding = create_word2vec(input_sequence, length) vector = map_token('TAA', embedding) print(vector) While we demonstrate gensim as one example, many embedding algorithms exist. A comprehensive overview of each method is out of the scope of this tutorial, but we list a few popular methods for your reference: Word2Vec dna2vec (dna equivalent of the above) GloVe END","title":"Tokenization"},{"location":"01_tokenization/handout/handout/#preprocessing-biological-sequence-data-for-natural-language-processing","text":"","title":"Preprocessing Biological Sequence Data for Natural Language Processing"},{"location":"01_tokenization/handout/handout/#learning-objectives","text":"The overall objective is to understand how biological sequence data is transformed into a machine-readable format for input into a machine learning pipeline. View the flow of data through a conventional DNA data preproceessing pipeline Familiarity with common genomic data transformations and representations Explore how large sequences are separated into smaller components Awareness of the ecosystem of available libraries","title":"Learning objectives:"},{"location":"01_tokenization/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"01_tokenization/handout/handout/#tools-used","text":"Python Package: Pandas, BioPython, sklearn, gensim BioPython SeqIO package: https://biopython.org/wiki/SeqIO Google Colaboratory: https://colab.research.google.com/","title":"Tools Used"},{"location":"01_tokenization/handout/handout/#useful-links","text":"","title":"Useful Links"},{"location":"01_tokenization/handout/handout/#author-information","text":"Primary Author(s): Tyrone Chen, Navya Tyagi, Naima Vahab Contributor(s): Sonika Tyagi","title":"Author Information"},{"location":"01_tokenization/handout/handout/#workflow","text":"Assuming you have obtained well-defined sequence data and their corresponding metadata: 1. Split each input sequence into \u201ck-mers\u201d or \u201ctokens\u201d. 2. Denoise the data if needed. 3. Convert these tokens into a numeric representation. 4. Transform the numeric representation as needed, depending on your use case. NOTE : in the context of this tutorial, we will be using the term token and k-mer interchangeably. Let us take a single biological sequence as an example. Assume that this corresponds to a gene of interest: input_sequence = \"TAATGGCTCT\"","title":"Workflow"},{"location":"01_tokenization/handout/handout/#1-split-each-input-sequence-into-tokens","text":"It is impractical in most cases for the machine to obtain a representation of a whole sequence, for example a whole document or chromosome. On the other hand, the same is true for obtaining individual letters of text or genomic DNA. While in the case of some languages such as English, it is possible to exploit the property of space-separation, this is not applicable to genomic DNA. Therefore, in most cases an arbitrary-length split is performed. def get_kmers(sequence: str, length: int): \"\"\" Take a dna sequence as input and split the sequence into k-mers / tokens \"\"\" return [sequence[x:x+length].upper() for x in range(len(sequence) - length + 1)] # in this example, we will split on length 3 k-mers length = 3 kmers = get_kmers(input_sequence, length) print(kmers) We can see that we get k-mers or tokens of length 3 in our example. It is important to note that we take a sliding window of k-mers: INPUT : TAATG... KMER_1: TAA-- KMER_2: -AAT- KMER_3: --ATG NOTE : in real applications, a range of longer k-mers are commonly used, and this is not the only way to split your data into chunks0","title":"1) Split each input sequence into \"tokens\"."},{"location":"01_tokenization/handout/handout/#2-denoise-the-data-if-needed","text":"In some cases, we want to remove certain tokens, commonly tokens with low information content. In English, this is straightforward and involves filtering out \u201cstopwords\u201d, for example the , and or is . In Biology, this is not always done, and will require review on a case-by-case basis. def filter_kmers(tokens: str, stopwords: list): \"\"\" Take an input dna sequence and list of stopwords to remove from the data. \"\"\" return [x for x in tokens if x not in stopwords] # in this example, let us pretend this list of k-mers have low information content stopwords = [\"TAA\", \"AAT\"] filtered_kmers = filter_kmers(kmers, stopwords) print(filtered_kmers)","title":"2) Denoise the data if needed"},{"location":"01_tokenization/handout/handout/#step-1-numeric-encoding","text":"Ordinal encoding One-hot encoding The first type of text encodings are ordinal encodings. With this, individual tokens are assigned an integer value as shown below: from sklearn.preprocessing import OrdinalEncoder def encode_ordinal(input_sequence: str, length: int): \"\"\" Take a list of k-mers and perform ordinal encoding \"\"\" tokens = [[x] for x in get_kmers(input_sequence, length)] encoder = OrdinalEncoder() return encoder.fit_transform(tokens) ordinal = encode_ordinal(input_sequence, length) print(ordinal) [[5.] [0.] [1.] [7.] [4.] [3.] [2.] [6.]] NOTE : As an alternative to ordinal encoding, it is also possible to encode tokens as one-hot encodings. This method is not practical to use in most cases due to its sparsity, but we demonstrate an example for completeness. Compare this to the above: from numpy import asarray from sklearn.preprocessing import OneHotEncoder def onehoten(sequence,size): r\"\"\"Array of categories(here string) are sorted and returns binary variables for each category\"\"\" doc=sentence_in_list(sequence,size) data = asarray(doc) encoder = OneHotEncoder(sparse=False) onehot = encoder.fit_transform(data) return onehot onehoten(input_sequence,length) [[ 0 0 0 0 0 1 0 0] [ 1 0 0 0 0 0 0 0] [ 0 1 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 1] [ 0 0 0 0 1 0 0 0] [ 0 0 0 1 0 0 0 0] [ 0 0 1 0 0 0 0 0] [ 0 0 0 0 0 0 1 0]] While these can be used on their own, note that this is often just the first step, and that ordinal encodings can be transformed further. While these can be used on their own, note that this is often just the first step, and that ordinal encodings can be transformed further.","title":"Step 1: Numeric encoding"},{"location":"01_tokenization/handout/handout/#step-2-transformation","text":"Frequency tables Embedding vector Positional encoding","title":"Step 2: Transformation"},{"location":"01_tokenization/handout/handout/#frequency-tables","text":"Count vectorisation First, each token is counted and these values are assigned to a dictionary. On its own, these counts can be vectorised directly. However, it is often practical to weight tokens by their frequency as shown in the next step: TF-IDF (Term Frequency-Inverse Document Frequency) TF-IDF is analogous to a library-size correction in conventional short-read RNA-Seq, allowing different size sequence collections to be compared on similar scales. It is computed on the previous counts prior to vectorisation. TF-IDF can be calculated as follows: tf-idf(t, d) = tf(t, d) * log(N/(df + 1)) Where: - t = token - d = sequence - N = total number of sequences in the library Within scikit-learn , the counting process is abstracted and not shown directly. We demonstrate this intermediate step for reference: # input 'TAA AAT ATG TGG GGC GCT CTC TCT' # intermediate (unweighted matrix) {'TAA': 1, 'AAT': 1, 'ATG': 1, 'TGG': 1, 'GGC': 1, 'GCT': 1, 'CTC': 1, 'TCT': 1} # resulting matrix from sklearn.feature_extraction.text import TfidfVectorizer def tfidf(input_sequence: str, length: int): \"\"\" Take a dna sequence and k-mer size as input, output a matrix of TF-IDF features. \"\"\" data = [\" \".join(get_kmers(input_sequence, length))] tfidf = TfidfVectorizer() tfidf = tfidf.fit_transform(data) return tfidf.toarray() tfidf_matrix = tfidf(input_sequence,length) print(tfidf_matrix) Although the frequency-based approaches above are often effective on their own, one key piece of information is lost. Individual tokens are treated as independent occurrences, which is not realistic in both natural language and biology. To account for the semantic relationship between tokens, a different approach is required.","title":"Frequency tables"},{"location":"01_tokenization/handout/handout/#embedding-vector","text":"A latent space is generated from all tokens in the corpus. This n-dimensional embedding accounts for every combination of tokens in the data. Taking a single sequence (i.e. group of tokens) and projecting this back onto the embedding returns a unique vector for each sequence. Examining these further shows that tokens which are closely related to each other are naturally recapitulated by the model. For example, in a real life English dataset, you may expect the following word pairs to be closely related to each other: import gensim from gensim.models import Word2Vec def create_word2vec(input_sequence: str, length: int): \"\"\" Input a list of tokens to generate an embedding of word vectors \"\"\" data = get_kmers(input_sequence, length) return Word2Vec([data], min_count=1) def map_token(input_sequence: str, model: gensim.models.base_any2vec): \"\"\" Input a token and embedding and map the token onto the embedding \"\"\" return model[input_sequence] embedding = create_word2vec(input_sequence, length) vector = map_token('TAA', embedding) print(vector) While we demonstrate gensim as one example, many embedding algorithms exist. A comprehensive overview of each method is out of the scope of this tutorial, but we list a few popular methods for your reference: Word2Vec dna2vec (dna equivalent of the above) GloVe","title":"Embedding vector"},{"location":"01_tokenization/handout/handout/#end","text":"","title":"END"},{"location":"02_RF_model/handout/handout/","text":"Building a Random Forest model (RF) to classify a set of sequences The objective of this exercise is to build a classiciation model that is training on two groups of sequences with known labels i.e. \u201cpromoter\u201d and \u201cnon-promoter\u201d. The model is then trained on a held out model to make prediction - whether the given sequence is a promoter or not. Learning objectives: At the end of this practical you should be able to: Prepare input data forms for a ML model (RF) Investigate parameters used in the model Prepare training and test performance metrices and visualization Resources You\u2019ll be Using Data We have promoter seuqneces of Yeast as one group of data labeled as 1 , and a similar synthetic negative data was generated and labelled as 0 . The data is provided in a file called promoter.csv Tools Used Python Package: pandas, numpym sklearn, matplotlib, seaborn Google Colaboratory: https://colab.research.google.com/ Useful Links Random Classifier https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Confusion matrix: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html A read on ROC curves https://aiineverything.blogspot.com/2021/08/misconception--around-roc-auc.html Author Information Primary Author(s): Sonika Tyagi, Navya Tyagi Contributor(s): Tyrone Chen, Sarthak Chauhan Workflow Assuming you have obtained well-defined sequence data and their corresponding metadata: Read in the data and split each input sequence into \u201ck-mers\u201d or \u201ctokens\u201d. Denoise the data if needed.[optional] Convert these tokens into a numeric representation. Split the data into train , test , and validation sets. Train, optimize, and validate the model. NOTE : in the context of this tutorial, we will be using the term token and k-mer interchangeably. First lets load the required python packages: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix from pprint import pprint import matplotlib.pyplot as plt import seaborn as sns from sklearn.feature_extraction.text import CountVectorizer 1) Read in the data and split each input sequence into \u201ctokens\u201d. We have prepared a data file ( .csv format) contain sequences from promoter (label 1 ) and non-promoter (label 0 ) groups. dna = pd.read_csv('/content/promoter.csv') dna.head() Lets check the size of the data: color = [sns.xkcd_rgb['medium blue'], sns.xkcd_rgb['pale red']] sns.countplot('labels',data = dna, palette = color) plt.gca().set_ylabel('Samples (number of sequences)') Here, is a resuable function to split sequences into k-mers: def get_kmers(sequence: str, length: int): \"\"\" Take a dna sequence as input and split the sequence into k-mers / tokens \"\"\" return [sequence[x:x+length].upper() for x in range(len(sequence) - length + 1)] Use the above function with the sequence data now stored in a dataframe named dna : # in this example, we will split on length 5 k-mers kmer_length = seq=dna.sequence[0] dna['kmers'] = dna.apply(lambda x: get_kmers(x['sequence'], kmer_length), axis=1) print(dna.head()) We have previously determined the length of kmer=5 to be used here. We will next join the individual kmers to long string of sentence equivalent. Use the following function: def to_sentence(words): r\"\"\"This function formats the k-mers such that it is useful for language models where algorithm expects sequences of words\"\"\" initial='' for element in range(len(words)): words[element]=' '.join(words[element]) return words First, extract the kmers column as a list and then perform the joining opration. dna_text = list(dna['kmers']) #get kmers/words joined together with space between them joined_kmers=to_sentence(dna_text) print(len(joined_kmers)) 2) Denoise the data if needed In some cases, we want to remove certain tokens, commonly tokens with low information content. In English, this is straightforward and involves filtering out \u201cstopwords\u201d, for example the , and or is . In Biology, this is not always done, and will require review on a case-by-case basis. def filter_kmers(tokens: str, stopwords: list): \"\"\" Take an input dna sequence and list of stopwords to remove from the data. \"\"\" return [x for x in tokens if x not in stopwords] # in this example, let us pretend this list of k-mers have low information content stopwords = [AAAAA\", \"TTTTT\"] filtered_kmers = filter_kmers(kmers, stopwords) print(filtered_kmers) 3) Convert data into numeric presentation Here, we are using a frequuency transformation based approach. The n-gram size of 4 is previously determined by testing cv = CountVectorizer(ngram_range=(,), max_features=500) X = cv.fit_transform(joined_kmers) #get feature names # X and Y should have same length #separate labels Y=np.array(dna['labels']) print(\"Total data iterms:\",X.shape) print(\"Total data labels\", Y.shape) Take a peek into the count vector we just created: cv.get_feature_names_out()[10:50] 4) Split the data into train, test, and validation sets def split_dataset(X, Y, train_ratio, test_ratio, validation_ratio): x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=1 - train_ratio) x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio)) return x_train, y_train, x_test, y_test, x_val, y_val # split the dataset into train, test and validation set using sklearn train_ratio = 0.70 validation_ratio = 0.15 test_ratio = 0.15 # train is now 70% of the entire data set # test is now 15% of the initial data set # validation is now 15% of the initial data set x_train, y_train, x_test, y_test, x_val, y_val=split_dataset(X, Y, train_ratio, test_ratio, validation_ratio) print(\"Training data:\",x_train.shape) print(\"Training data labels:\",y_train.shape) print(\"Test data:\",x_test.shape) print(\"Test data labels:\",y_test.shape) print(\"Validation data:\",x_val.shape) print(\"Validation data labels:\",y_val.shape) Functions to run training and perform assessment def train_model(model, param, x_train, y_train, x_test): clf=model(n_estimators=param) # fit the training data into the model clf.fit(x_train, y_train) y_pred=clf.predict(x_test) y_probas=clf.predict_proba(x_test) return clf, y_pred, y_probas def model_metrics(model, x_test, y_test, y_pred): # accuracy prediction accuracy = accuracy_score(y_test, y_pred) print(\"Accuracy: %.2f%%\" % (accuracy * 100.0)) # classification report print(\"Classification report:\\n\") print(classification_report(y_test, y_pred)) # confusion matrix conf=confusion_matrix(y_test, y_pred) print(\"Confusion matrix:\\n\", conf) Random Forest classication We have two steps here: 1) we train the model 2) we test it on unseen data print('RF BASE MODEL') ## training the model model=RandomForestClassifier param=100 rf_base, y_pred, y_probas=train_model(model, param, x_train, y_train, x_test) # model metrics model_metrics(rf_base, x_test, y_test, y_pred) Lets plot a ROC-AUC curve: from sklearn.metrics import RocCurveDisplay ax = plt.gca() rfc_disp = RocCurveDisplay.from_estimator(rf_base, x_test, y_test, ax=ax, alpha=0.8) #rfc_disp.plot(ax=ax, alpha=0.8) plt.show() For this exercise we have run the model with default paramters but in real practice we will running and optimisation step where we will find the best paramter settings for pur case study. We will discuss that topic on another day! The learning plots can give an idea whether the model is over or underfitting: from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt from matplotlib.legend_handler import HandlerLine2D train_results = [] test_results = [] list_nb_trees = [5, 10, 15, 30, 45, 60, 80, 100] for nb_trees in list_nb_trees: rf = RandomForestRegressor(n_estimators=nb_trees) rf.fit(x_train, y_train) train_results.append(mean_squared_error(y_train, rf.predict(x_train))) test_results.append(mean_squared_error(y_test, rf.predict(x_test))) line1, = plt.plot(list_nb_trees, train_results, color=\"r\", label=\"Training Score\") line2, = plt.plot(list_nb_trees, test_results, color=\"g\", label=\"Testing Score\") plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)}) plt.ylabel('MSE') plt.xlabel('n_estimators') plt.show() END","title":"ML model"},{"location":"02_RF_model/handout/handout/#building-a-random-forest-model-rf-to-classify-a-set-of-sequences","text":"The objective of this exercise is to build a classiciation model that is training on two groups of sequences with known labels i.e. \u201cpromoter\u201d and \u201cnon-promoter\u201d. The model is then trained on a held out model to make prediction - whether the given sequence is a promoter or not.","title":"Building a Random Forest model (RF) to classify a set of sequences"},{"location":"02_RF_model/handout/handout/#learning-objectives","text":"At the end of this practical you should be able to: Prepare input data forms for a ML model (RF) Investigate parameters used in the model Prepare training and test performance metrices and visualization","title":"Learning objectives:"},{"location":"02_RF_model/handout/handout/#resources-youll-be-using","text":"","title":"Resources You\u2019ll be Using"},{"location":"02_RF_model/handout/handout/#data","text":"We have promoter seuqneces of Yeast as one group of data labeled as 1 , and a similar synthetic negative data was generated and labelled as 0 . The data is provided in a file called promoter.csv","title":"Data"},{"location":"02_RF_model/handout/handout/#tools-used","text":"Python Package: pandas, numpym sklearn, matplotlib, seaborn Google Colaboratory: https://colab.research.google.com/","title":"Tools Used"},{"location":"02_RF_model/handout/handout/#useful-links","text":"Random Classifier https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Confusion matrix: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html A read on ROC curves https://aiineverything.blogspot.com/2021/08/misconception--around-roc-auc.html","title":"Useful Links"},{"location":"02_RF_model/handout/handout/#author-information","text":"Primary Author(s): Sonika Tyagi, Navya Tyagi Contributor(s): Tyrone Chen, Sarthak Chauhan","title":"Author Information"},{"location":"02_RF_model/handout/handout/#workflow","text":"Assuming you have obtained well-defined sequence data and their corresponding metadata: Read in the data and split each input sequence into \u201ck-mers\u201d or \u201ctokens\u201d. Denoise the data if needed.[optional] Convert these tokens into a numeric representation. Split the data into train , test , and validation sets. Train, optimize, and validate the model. NOTE : in the context of this tutorial, we will be using the term token and k-mer interchangeably. First lets load the required python packages: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix from pprint import pprint import matplotlib.pyplot as plt import seaborn as sns from sklearn.feature_extraction.text import CountVectorizer","title":"Workflow"},{"location":"02_RF_model/handout/handout/#1-read-in-the-data-and-split-each-input-sequence-into-tokens","text":"We have prepared a data file ( .csv format) contain sequences from promoter (label 1 ) and non-promoter (label 0 ) groups. dna = pd.read_csv('/content/promoter.csv') dna.head() Lets check the size of the data: color = [sns.xkcd_rgb['medium blue'], sns.xkcd_rgb['pale red']] sns.countplot('labels',data = dna, palette = color) plt.gca().set_ylabel('Samples (number of sequences)') Here, is a resuable function to split sequences into k-mers: def get_kmers(sequence: str, length: int): \"\"\" Take a dna sequence as input and split the sequence into k-mers / tokens \"\"\" return [sequence[x:x+length].upper() for x in range(len(sequence) - length + 1)] Use the above function with the sequence data now stored in a dataframe named dna : # in this example, we will split on length 5 k-mers kmer_length = seq=dna.sequence[0] dna['kmers'] = dna.apply(lambda x: get_kmers(x['sequence'], kmer_length), axis=1) print(dna.head()) We have previously determined the length of kmer=5 to be used here. We will next join the individual kmers to long string of sentence equivalent. Use the following function: def to_sentence(words): r\"\"\"This function formats the k-mers such that it is useful for language models where algorithm expects sequences of words\"\"\" initial='' for element in range(len(words)): words[element]=' '.join(words[element]) return words First, extract the kmers column as a list and then perform the joining opration. dna_text = list(dna['kmers']) #get kmers/words joined together with space between them joined_kmers=to_sentence(dna_text) print(len(joined_kmers))","title":"1) Read in the data and split each input sequence into \"tokens\"."},{"location":"02_RF_model/handout/handout/#2-denoise-the-data-if-needed","text":"In some cases, we want to remove certain tokens, commonly tokens with low information content. In English, this is straightforward and involves filtering out \u201cstopwords\u201d, for example the , and or is . In Biology, this is not always done, and will require review on a case-by-case basis. def filter_kmers(tokens: str, stopwords: list): \"\"\" Take an input dna sequence and list of stopwords to remove from the data. \"\"\" return [x for x in tokens if x not in stopwords] # in this example, let us pretend this list of k-mers have low information content stopwords = [AAAAA\", \"TTTTT\"] filtered_kmers = filter_kmers(kmers, stopwords) print(filtered_kmers)","title":"2) Denoise the data if needed"},{"location":"02_RF_model/handout/handout/#3-convert-data-into-numeric-presentation","text":"Here, we are using a frequuency transformation based approach. The n-gram size of 4 is previously determined by testing cv = CountVectorizer(ngram_range=(,), max_features=500) X = cv.fit_transform(joined_kmers) #get feature names # X and Y should have same length #separate labels Y=np.array(dna['labels']) print(\"Total data iterms:\",X.shape) print(\"Total data labels\", Y.shape) Take a peek into the count vector we just created: cv.get_feature_names_out()[10:50]","title":"3) Convert data into numeric presentation"},{"location":"02_RF_model/handout/handout/#4-split-the-data-into-train-test-and-validation-sets","text":"def split_dataset(X, Y, train_ratio, test_ratio, validation_ratio): x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=1 - train_ratio) x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio)) return x_train, y_train, x_test, y_test, x_val, y_val # split the dataset into train, test and validation set using sklearn train_ratio = 0.70 validation_ratio = 0.15 test_ratio = 0.15 # train is now 70% of the entire data set # test is now 15% of the initial data set # validation is now 15% of the initial data set x_train, y_train, x_test, y_test, x_val, y_val=split_dataset(X, Y, train_ratio, test_ratio, validation_ratio) print(\"Training data:\",x_train.shape) print(\"Training data labels:\",y_train.shape) print(\"Test data:\",x_test.shape) print(\"Test data labels:\",y_test.shape) print(\"Validation data:\",x_val.shape) print(\"Validation data labels:\",y_val.shape) Functions to run training and perform assessment def train_model(model, param, x_train, y_train, x_test): clf=model(n_estimators=param) # fit the training data into the model clf.fit(x_train, y_train) y_pred=clf.predict(x_test) y_probas=clf.predict_proba(x_test) return clf, y_pred, y_probas def model_metrics(model, x_test, y_test, y_pred): # accuracy prediction accuracy = accuracy_score(y_test, y_pred) print(\"Accuracy: %.2f%%\" % (accuracy * 100.0)) # classification report print(\"Classification report:\\n\") print(classification_report(y_test, y_pred)) # confusion matrix conf=confusion_matrix(y_test, y_pred) print(\"Confusion matrix:\\n\", conf)","title":"4) Split the data into train, test, and validation sets"},{"location":"02_RF_model/handout/handout/#random-forest-classication","text":"We have two steps here: 1) we train the model 2) we test it on unseen data print('RF BASE MODEL') ## training the model model=RandomForestClassifier param=100 rf_base, y_pred, y_probas=train_model(model, param, x_train, y_train, x_test) # model metrics model_metrics(rf_base, x_test, y_test, y_pred) Lets plot a ROC-AUC curve: from sklearn.metrics import RocCurveDisplay ax = plt.gca() rfc_disp = RocCurveDisplay.from_estimator(rf_base, x_test, y_test, ax=ax, alpha=0.8) #rfc_disp.plot(ax=ax, alpha=0.8) plt.show() For this exercise we have run the model with default paramters but in real practice we will running and optimisation step where we will find the best paramter settings for pur case study. We will discuss that topic on another day! The learning plots can give an idea whether the model is over or underfitting: from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt from matplotlib.legend_handler import HandlerLine2D train_results = [] test_results = [] list_nb_trees = [5, 10, 15, 30, 45, 60, 80, 100] for nb_trees in list_nb_trees: rf = RandomForestRegressor(n_estimators=nb_trees) rf.fit(x_train, y_train) train_results.append(mean_squared_error(y_train, rf.predict(x_train))) test_results.append(mean_squared_error(y_test, rf.predict(x_test))) line1, = plt.plot(list_nb_trees, train_results, color=\"r\", label=\"Training Score\") line2, = plt.plot(list_nb_trees, test_results, color=\"g\", label=\"Testing Score\") plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)}) plt.ylabel('MSE') plt.xlabel('n_estimators') plt.show()","title":"Random Forest classication"},{"location":"02_RF_model/handout/handout/#end","text":"","title":"END"},{"location":"trainers/trainers/","text":"Trainers Info here","title":"The Trainers"},{"location":"trainers/trainers/#trainers-info-here","text":"","title":"Trainers Info here"}]}